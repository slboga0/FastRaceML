{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Horse Race Data Pipeline Notebook\n",
    "\n",
    "This notebook documents our Horse Race Data Pipeline. It explains what the pipeline does, how it works, and suggests future improvements. The notebook also demonstrates key steps such as data ingestion, merging, feature engineering, and model preparation.\n",
    "\n",
    "Below is an overview of the pipeline architecture."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Architecture & Workflow Diagram\n",
    "\n",
    "```plaintext\n",
    "    +------------------------+\n",
    "    |   Configuration &      |\n",
    "    |   Setup (config.ini)   |\n",
    "    +-----------+------------+\n",
    "                |\n",
    "                v\n",
    "    +-----------+------------+\n",
    "    | Data Ingestion         |\n",
    "    |  - PP Files (scan_cards, load_all_pp_cards)\n",
    "    |  - Result Files (load_results_and_merge)\n",
    "    +-----------+------------+\n",
    "                |\n",
    "                v\n",
    "    +-----------+------------+\n",
    "    | Data Merging           |\n",
    "    |  - Build merge key     |\n",
    "    |  - Merge PP & Results  |\n",
    "    +-----------+------------+\n",
    "                |\n",
    "                v\n",
    "    +-----------+------------+\n",
    "    | Feature Engineering    |\n",
    "    |  - Select predictive   |\n",
    "    |    features            |\n",
    "    |  - Impute missing vals |\n",
    "    |  - Save engineered CSV |\n",
    "    +-----------+------------+\n",
    "                |\n",
    "                v\n",
    "    +-----------+------------+\n",
    "    | Model Preparation &    |\n",
    "    | Training (Optional)    |\n",
    "    +------------------------+\n",
    "```\n",
    "\n",
    "This diagram shows the sequential flow from configuration, through data ingestion, merging, feature engineering, and finally to model training."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 1: Configuration and Setup\n",
    "\n",
    "The pipeline reads configuration settings (such as the track code and file paths) from a configuration file (`config.ini`) and sets up logging. This allows the pipeline to know which directories to look at for the Past Performance (PP) and result files and how to log its progress."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'data_prep'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[2], line 2\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;66;03m# Example: Load configuration and setup logging\u001b[39;00m\n\u001b[1;32m----> 2\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mdata_prep\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mutilities\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m load_config, setup_logging\n\u001b[0;32m      3\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mlogging\u001b[39;00m\n\u001b[0;32m      5\u001b[0m setup_logging(level\u001b[38;5;241m=\u001b[39mlogging\u001b[38;5;241m.\u001b[39mINFO)\n",
      "\u001b[1;31mModuleNotFoundError\u001b[0m: No module named 'data_prep'"
     ]
    }
   ],
   "source": [
    "# Example: Load configuration and setup logging\n",
    "from data_prep.utilities import load_config, setup_logging\n",
    "import logging\n",
    "\n",
    "setup_logging(level=logging.INFO)\n",
    "config = load_config()\n",
    "track = config['DEFAULT'].get('track', 'SA')\n",
    "print(f\"Track from config.ini: {track}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 2: Data Ingestion and Merging\n",
    "\n",
    "### Past Performance (PP) Data\n",
    "\n",
    "- **scan_cards()** scans the given PP directory (non-recursively) for files matching a given pattern (e.g., `SA*.DRF`).\n",
    "- **load_all_pp_cards()** loads the PP data from the scanned files, using a PP field mapping CSV.\n",
    "\n",
    "### Race Results Data\n",
    "\n",
    "- The result field mapping CSV (e.g., `result_field_mapping.csv`) defines the structure of the result files.\n",
    "- **load_results_and_merge()** parses the result files, constructs a result DataFrame, and merges it with the PP DataFrame using a common merge key (`race_key`).\n",
    "\n",
    "The merged data is saved to `merged_data.csv`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example: Ingest and merge data\n",
    "from data_prep.ingestion import scan_cards, load_all_pp_cards, load_results_and_merge\n",
    "\n",
    "# Get directories and mapping paths from config\n",
    "pp_location = config['DEFAULT'].get('pp_location')\n",
    "result_location = config['DEFAULT'].get('result_location')\n",
    "pp_field_mapping_csv = config['DEFAULT'].get('pp_fields_mapping_location')\n",
    "result_field_mapping_csv = config['DEFAULT'].get('result_fields_mapping_location')\n",
    "\n",
    "# Build file patterns based on track code\n",
    "pp_track_pattern = f\"{track}*.DRF\"\n",
    "res_track_pattern = f\"{track}*.*\"\n",
    "\n",
    "# Load PP data\n",
    "pp_cards = scan_cards(pp_location, pp_track_pattern)\n",
    "pp_all_df, pp_field_map = load_all_pp_cards(pp_cards, pp_field_mapping_csv)\n",
    "print(\"Loaded PP data shape:\", pp_all_df.shape)\n",
    "\n",
    "# Load result data and merge with PP data\n",
    "merged_df, result_field_map = load_results_and_merge(pp_all_df, result_location, result_field_mapping_csv, res_track_pattern)\n",
    "print(\"Merged data shape:\", merged_df.shape)\n",
    "print(\"Merged columns:\", merged_df.columns.tolist())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 3: Feature Engineering\n",
    "\n",
    "The feature engineering step extracts predictive features from both the result and PP field mappings. It then selects only those columns from the merged DataFrame. Instead of dropping rows with missing values, the pipeline imputes them (e.g., with the median for numeric columns and a placeholder for categorical columns). Finally, the engineered DataFrame is saved to `engineered_features.csv`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example: Feature Engineering\n",
    "from feature_engineering import engineer_features\n",
    "\n",
    "# Call the feature engineering function\n",
    "engineered_df = engineer_features(merged_df, result_field_map, pp_field_map)\n",
    "print(\"Engineered features shape:\", engineered_df.shape)\n",
    "engineered_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 4: Model Preparation and Basic ML Example\n",
    "\n",
    "Once we have the engineered features, we prepare the data for modeling. This step typically involves splitting the data into training, validation, and test sets and then training a model. In the example below, we demonstrate this with a simple linear regression model to predict the race finish position (assumed to be in the column `res_finish_position`).\n",
    "\n",
    "Adjust the target column and model as necessary."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.metrics import mean_squared_error\n",
    "\n",
    "# Set the target column. Adjust if needed.\n",
    "target_column = \"res_finish_position\"\n",
    "\n",
    "# Ensure the target column exists in engineered_df\n",
    "if target_column not in engineered_df.columns:\n",
    "    raise ValueError(f\"Target column '{target_column}' not found. Available columns: {engineered_df.columns.tolist()}\")\n",
    "\n",
    "# Separate features and target\n",
    "X = engineered_df.drop(columns=[target_column])\n",
    "y = engineered_df[target_column]\n",
    "\n",
    "# Split data into training and test sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Train a simple linear regression model\n",
    "model = LinearRegression()\n",
    "model.fit(X_train, y_train)\n",
    "\n",
    "# Predict on the test set and compute MSE\n",
    "y_pred = model.predict(X_test)\n",
    "mse = mean_squared_error(y_test, y_pred)\n",
    "print(f\"Mean Squared Error: {mse}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 5: Future Improvements and Roadmap\n",
    "\n",
    "- **Data Ingestion:**\n",
    "  - Support for additional data formats (JSON, XML).\n",
    "  - Enhanced error handling and logging.\n",
    "\n",
    "- **Data Cleaning & Feature Engineering:**\n",
    "  - Advanced imputation strategies (e.g., kNN imputation).\n",
    "  - Outlier detection and removal.\n",
    "  - Feature scaling and normalization.\n",
    "\n",
    "- **Modeling Enhancements:**\n",
    "  - Experiment with ensemble methods (e.g., Random Forest, Gradient Boosting).\n",
    "  - Hyperparameter tuning and cross-validation.\n",
    "  - Explore deep learning if data volume permits.\n",
    "\n",
    "- **Pipeline Automation & Deployment:**\n",
    "  - Containerization for reproducibility.\n",
    "  - Workflow orchestration with tools like Airflow or Prefect.\n",
    "  - Real-time prediction dashboards.\n",
    "\n",
    "- **Collaboration & Documentation:**\n",
    "  - Detailed documentation and code comments.\n",
    "  - Interactive notebooks for exploratory data analysis."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Summary\n",
    "\n",
    "- We built a comprehensive pipeline that ingests, merges, and processes horse race data.\n",
    "- Feature engineering is performed by selecting only predictive features (as defined by the field mappings) and imputing missing values.\n",
    "- A basic ML example demonstrates model preparation and training.\n",
    "- Future improvements include better data cleaning, advanced modeling, and deployment strategies."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
